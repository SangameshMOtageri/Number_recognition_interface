{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\india\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /Downloads/mnists\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /Downloads/mnists\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /Downloads/mnists\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /Downloads/mnists\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets('/Downloads/mnists', one_hot='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_1 = 5\n",
    "channels_1 = 16\n",
    "\n",
    "filter_2 = 5\n",
    "channels_2 = 36\n",
    "\n",
    "fc_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_test_cls = np.argmax(data.test.labels, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'weights_biases/CNN_mnist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsize = 28*28\n",
    "input_channels = 1\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test.cls = np.argmax(data.test.labels, axis=1)\n",
    "data.validation.cls = np.argmax(data.validation.labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(images, cls_true, cls_pred = None):\n",
    "    fig, axes = plt.subplots(1,2)\n",
    "    fig.subplots_adjust(hspace = 0.3, wspace = 0.3)\n",
    "    for i,ax in enumerate(axes.flat):\n",
    "        if cls_pred is not None:\n",
    "            xlabel = 'True:{0} and Pred:{1}'.format(cls_true[i], cls_pred[i])\n",
    "        else:\n",
    "            xlabel = 'True:{0} '.format(cls_true[i])\n",
    "        ax.imshow(images[i].reshape(28, 28), cmap='binary')\n",
    "        ax.set_xlabel(xlabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_test = data.test.images[1:3]\n",
    "cls_true = y_true_test_cls[1:3]\n",
    "pred = np.array([7,2])\n",
    "plot_image(images=image_test, cls_true=cls_true, cls_pred=pred)\n",
    "print(image_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev = 0.05))\n",
    "def new_biases(biaslength):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[biaslength]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_conv_layer(input_image,\n",
    "              num_channels,#number of channels in the previous layer output(Ex:16)\n",
    "              filter_size,#usually 5x5\n",
    "              num_filters,#number of channels coming out of this layer(Ex:36)\n",
    "              pooling = True):\n",
    "    shape = [filter_size, filter_size, num_channels, num_filters]\n",
    "    weights = new_weights(shape)\n",
    "    biases = new_biases(biaslength = num_filters)\n",
    "    \n",
    "    layer = tf.nn.conv2d(input = input_image,\n",
    "                        filter = weights,\n",
    "                        strides = [1,1,1,1],#input_imgae_number, x_axis, y_Axis, channels in use.change only middle two\n",
    "                        padding = 'SAME')\n",
    "    layer += biases\n",
    "    \n",
    "    if pooling:\n",
    "        layer = tf.nn.max_pool(value = layer,\n",
    "                              ksize = [1,2,2,1],#Size of window to be considered(window from which we pick the max value)\n",
    "                              strides = [1,2,2,1],#Usually the same as the window size\n",
    "                              padding = 'SAME')\n",
    "    layer = tf.nn.relu(layer)\n",
    "    \n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    #Usually the conv layer returns a 4 dim data, we need to flatten to 2 dim before giving it to FC layer.\n",
    "    layer_shape = layer.shape\n",
    "    #Expected: [num_images, img_width, imag_height, num_channels]\n",
    "    num_features = np.array(layer_shape[1:4], dtype=int).prod()\n",
    "    #transform to 2dim, [num_images, num_features]\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are expecting the input to be a 2 dimensional data.[num_images, num_inputs==num_features].\n",
    "#Output will be again a 2 dim[num_images, num_outputs]\n",
    "def fully_connected_layer(images,#input from previous layer\n",
    "                         num_inputs,\n",
    "                         num_outputs,\n",
    "                         relu = True):\n",
    "    weights = new_weights(shape = [num_inputs, num_outputs])\n",
    "    biases = new_biases(biaslength = num_outputs)\n",
    "    \n",
    "    layer = tf.matmul(images, weights) + biases\n",
    "    \n",
    "    if relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, imgsize], name='x')\n",
    "#We are going to be givign the entire image data to this variable. meaning num_images X all pixels.\n",
    "x_image = tf.reshape(x, [-1, 28, 28, input_channels])\n",
    "#For feeding into the conv layer. [num_images, image_x_size, image_y_size, num_of_channels]\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "#[num_images, num_classes]\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1, weights_1 = new_conv_layer(input_image = x_image,\n",
    "                                      num_channels = input_channels,#number of channels in the previous layer output(Ex:16)\n",
    "                                      filter_size = filter_1,#usually 5x5\n",
    "                                      num_filters = channels_1,#number of channels coming out of this layer(Ex:36)\n",
    "                                      pooling = True)\n",
    "#We are actually providing all the images at once here. To understand see that x_image is of [num_images, 28, 28, 1=gray_image]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_8:0' shape=(5, 5, 1, 16) dtype=float32_ref>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2, weights_2 = new_conv_layer(input_image = layer_1,#This is ok, becasue we are providing conv layer with 4 dim data\n",
    "                                      num_channels = channels_1,#number of channels in the previous layer output(Ex:16)\n",
    "                                      filter_size = filter_2,#usually 5x5\n",
    "                                      num_filters = channels_2,#number of channels coming out of this layer(Ex:36)\n",
    "                                      pooling = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_4:0' shape=(?, 7, 7, 36) dtype=float32>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_10:0' shape=(5, 5, 16, 36) dtype=float32_ref>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_flat, num_features = flatten_layer(layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_3:0' shape=(?, 1764) dtype=float32>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use flatten API, because without it multiplication in the fully connected layer would be ver hard\n",
    "layer_fc_1 = fully_connected_layer(images = layer_flat,#input from the last conv layer.[num_images, num_features]\n",
    "                         num_inputs = num_features,\n",
    "                         num_outputs = fc_size,#This is the number of nodes in the first fully connected layer\n",
    "                         relu = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_5:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_fc_2 = fully_connected_layer(images = layer_fc_1,#input from the last conv layer.[num_images, num_features]\n",
    "                         num_inputs = fc_size,\n",
    "                         num_outputs = num_classes,#This is the number of nodes in the first fully connected layer\n",
    "                         relu = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_7:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer_fc_2 gives us the likely hood of each image belonging to a particular class.Now lets normalize it to get some readability\n",
    "#Note layer_fc_2 could be producing data which is too large or too small as well. Now we put that to some readable level.\n",
    "pred = tf.nn.softmax(layer_fc_2)\n",
    "y_pred_cls = tf.argmax(pred, dimension = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ArgMax_3:0' shape=(?,) dtype=int64>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section is to evaluate performance of each iteration. The cost\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = layer_fc_2,\n",
    "                                              labels = y_true)\n",
    "#Here cross entropy is calculated for each image. That has to be normalized over all images\n",
    "\n",
    "cost = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate = 1e-4).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "#correct_prediction wil be true or false. We first convert it to 1 or 0. The we take the mean over all input images\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_all():\n",
    "    session.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 100\n",
    "total_iterations = 0\n",
    "max_validation_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_the_model(num_iterations):\n",
    "    global total_iterations\n",
    "    global max_validation_acc\n",
    "    for i in range(num_iterations):\n",
    "        x_batch, y_batch = data.train.next_batch(batch_size = train_batch_size)\n",
    "        \n",
    "        feed_dict_train = {x: x_batch,\n",
    "                          y_true: y_batch}\n",
    "        \n",
    "        session.run(optimizer, feed_dict = feed_dict_train)\n",
    "        \n",
    "        total_iterations = total_iterations + 1\n",
    "        if total_iterations % 100 == 0:\n",
    "            \n",
    "            acc = session.run(accuracy, feed_dict = feed_dict_train)\n",
    "            validation_acc = get_validation_accuracy()\n",
    "            testing_acc = get_test_accuracy()\n",
    "            print('Iterations: ',total_iterations,' Training_accuracy: ',acc, ' Testing_accuracy: ',testing_acc, \n",
    "                 ' Validation_accuracy: ', validation_acc)\n",
    "            if validation_acc < max_validation_acc:\n",
    "                if  violation_before == 5:\n",
    "                    print('Max_validation accuracy: ', max_validation_acc)\n",
    "                    i = num_iterations\n",
    "                else:\n",
    "                    violation_before = violation_before + 1\n",
    "            else:\n",
    "                violation_before = 0\n",
    "                max_validation_acc = validation_acc\n",
    "                saver.save(sess=session, save_path=save_path)\n",
    "    acc = session.run(accuracy, feed_dict = feed_dict_train)\n",
    "    validation_acc = get_validation_accuracy()\n",
    "    testing_acc = get_test_accuracy()\n",
    "    print('Iterations: ',total_iterations,' Training_accuracy: ',acc, ' Testing_accuracy: ',testing_acc, \n",
    "                 ' Validation_accuracy: ', validation_acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_accuracy():\n",
    "    return print_accuracy(data.test.images,\n",
    "                  data.test.labels,\n",
    "                  data.test.cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_accuracy():\n",
    "    return print_accuracy(data.validation.images,\n",
    "                  data.validation.labels,\n",
    "                  data.validation.cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 256\n",
    "def print_accuracy(dataimages, datalabels, true_datalabels):\n",
    "    num_test_examples = len(dataimages)\n",
    "    cls_pred = np.zeros(shape=num_test_examples, dtype=np.int)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < num_test_examples:\n",
    "        #print('running ', (num_test_examples-i))\n",
    "        j = min(test_batch_size + i, num_test_examples)\n",
    "        images = dataimages[i:j, :]\n",
    "        labels = datalabels[i:j, :]\n",
    "        feed_dict_test = {x: images,\n",
    "                         y_true: labels}\n",
    "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict = feed_dict_test)\n",
    "        i = j\n",
    "    cls_true = true_datalabels\n",
    "    #print(cls_true.shape)\n",
    "    #print(cls_pred.shape)\n",
    "    correct = (cls_true == cls_pred)\n",
    "    acc = correct.sum()/num_test_examples\n",
    "    print('accuracy: ',format(acc*100),'%')\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  91.72 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9172"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  91.56 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9156"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_validation_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  83.82 %\n",
      "accuracy:  84.36 %\n",
      "Iterations:  200  Training_accuracy:  0.81  Testing_accuracy:  0.8436  Validation_accuracy:  0.8382\n",
      "accuracy:  87.36 %\n",
      "accuracy:  88.03999999999999 %\n",
      "Iterations:  300  Training_accuracy:  0.87  Testing_accuracy:  0.8804  Validation_accuracy:  0.8736\n",
      "accuracy:  89.14 %\n",
      "accuracy:  89.73 %\n",
      "Iterations:  400  Training_accuracy:  0.84  Testing_accuracy:  0.8973  Validation_accuracy:  0.8914\n",
      "accuracy:  91.16 %\n",
      "accuracy:  91.25999999999999 %\n",
      "Iterations:  500  Training_accuracy:  0.93  Testing_accuracy:  0.9126  Validation_accuracy:  0.9116\n",
      "accuracy:  91.56 %\n",
      "accuracy:  91.72 %\n",
      "Iterations:  600  Training_accuracy:  0.93  Testing_accuracy:  0.9172  Validation_accuracy:  0.9156\n",
      "accuracy:  91.56 %\n",
      "accuracy:  91.72 %\n",
      "Iterations:  600  Training_accuracy:  0.93  Testing_accuracy:  0.9172  Validation_accuracy:  0.9156\n"
     ]
    }
   ],
   "source": [
    "run_the_model(num_iterations = 500)#change to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_orig():\n",
    "    img = cv2.imread('C:/Users/india/Downloads/image.png', 0).astype(np.float32)\n",
    "    #img = 255-img\n",
    "    img = img/255\n",
    "    #img = img*255\n",
    "    \n",
    "    print('calling for image')\n",
    "    #img.dtype='float32'\n",
    "    resized_image = cv2.resize(img, (28,28))\n",
    "    print(resized_image.shape)\n",
    "    plt.imshow(resized_image, cmap='gray')\n",
    "    resized_image = resized_image.reshape(1,784)\n",
    "    print(resized_image.shape)\n",
    "    #os.remove('C:/Users/india/Downloads/image.png')\n",
    "\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling for image\n",
      "(28, 28)\n",
      "(1, 784)\n",
      "Prediction:  [2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_data = get_image_orig()\n",
    "print('Prediction: ',session.run(y_pred_cls, feed_dict = {x: input_data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(5, 5, 1, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_1:0' shape=(16,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_2:0' shape=(5, 5, 16, 36) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_3:0' shape=(36,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_4:0' shape=(1764, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_5:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_6:0' shape=(128, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_7:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'Variable/Adam:0' shape=(5, 5, 1, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable/Adam_1:0' shape=(5, 5, 1, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_1/Adam:0' shape=(16,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_1/Adam_1:0' shape=(16,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_2/Adam:0' shape=(5, 5, 16, 36) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_2/Adam_1:0' shape=(5, 5, 16, 36) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_3/Adam:0' shape=(36,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_3/Adam_1:0' shape=(36,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_4/Adam:0' shape=(1764, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_4/Adam_1:0' shape=(1764, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_5/Adam:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_5/Adam_1:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_6/Adam:0' shape=(128, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_6/Adam_1:0' shape=(128, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_7/Adam:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_7/Adam_1:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_8:0' shape=(5, 5, 1, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_9:0' shape=(16,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_10:0' shape=(5, 5, 16, 36) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_11:0' shape=(36,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_12:0' shape=(1764, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_13:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_14:0' shape=(128, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_15:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Variable 'beta1_power_1:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'beta2_power_1:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_8/Adam:0' shape=(5, 5, 1, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_8/Adam_1:0' shape=(5, 5, 1, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_9/Adam:0' shape=(16,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_9/Adam_1:0' shape=(16,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_10/Adam:0' shape=(5, 5, 16, 36) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_10/Adam_1:0' shape=(5, 5, 16, 36) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_11/Adam:0' shape=(36,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_11/Adam_1:0' shape=(36,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_12/Adam:0' shape=(1764, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_12/Adam_1:0' shape=(1764, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_13/Adam:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_13/Adam_1:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_14/Adam:0' shape=(128, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_14/Adam_1:0' shape=(128, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_15/Adam:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_15/Adam_1:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from weights_biases/CNN_mnist\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess=session, save_path=save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
